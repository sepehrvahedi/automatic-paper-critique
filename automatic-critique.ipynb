{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a3f923-2378-41d9-9384-3c4d7e5d5085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venv/lib/python3.9/site-packages (1.97.1)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.9/site-packages (11.3.0)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in ./venv/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in ./venv/lib/python3.9/site-packages (6.2.0)\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.9/site-packages (8.1.7)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: nbformat in ./venv/lib/python3.9/site-packages (5.10.4)\n",
      "Requirement already satisfied: google-generativeai in ./venv/lib/python3.9/site-packages (0.8.5)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.9/site-packages (4.13.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.9/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.9/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./venv/lib/python3.9/site-packages (from plotly) (1.48.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./venv/lib/python3.9/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./venv/lib/python3.9/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.9/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./venv/lib/python3.9/site-packages (from nbformat) (4.25.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.9/site-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./venv/lib/python3.9/site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in ./venv/lib/python3.9/site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.9/site-packages (from google-generativeai) (6.31.1)\n",
      "Requirement already satisfied: google-api-core in ./venv/lib/python3.9/site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in ./venv/lib/python3.9/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-python-client in ./venv/lib/python3.9/site-packages (from google-generativeai) (2.177.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.9/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2; python_version < \"3.11\" in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (0.26.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in ./venv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./venv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in ./venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.9/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./venv/lib/python3.9/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/sepehr/IdeaProjects/automatic-paper-critique/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "✅ All dependencies installed and imported successfully!\n",
      "📁 Directory structure created!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Installation and Imports\n",
    "!pip install openai pillow matplotlib seaborn plotly ipywidgets tqdm numpy pandas nbformat google-generativeai requests beautifulsoup4\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create results directory structure\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./results/individual_responses', exist_ok=True)\n",
    "os.makedirs('./results/component_analysis', exist_ok=True)\n",
    "\n",
    "print(\"✅ All dependencies installed and imported successfully!\")\n",
    "print(\"📁 Directory structure created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df6553d-8f57-49a1-826f-82aec0db2247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration and API Setup\n",
    "class LLMConfig:\n",
    "    def __init__(self):\n",
    "        # API Configuration\n",
    "        self.openai_api_key = \"sk-JdU36bC7BG2996XHH3YmKOQG8Xm9x9ii5u5E9uwPC54oAkHE\"\n",
    "        self.openai_base_url = \"https://api.gapgpt.app/v1\"\n",
    "        \n",
    "        # LLM Models Configuration\n",
    "        self.models = {\n",
    "            \"gpt\": {\n",
    "                \"name\": \"o4-mini\",\n",
    "                \"model_id\": \"o4-mini\",\n",
    "                \"filename\": \"o4_mini_response.md\"\n",
    "            },\n",
    "            \"gemini\": {\n",
    "                \"name\": \"Gemini 2.0 Flash\",\n",
    "                \"model_id\": \"gemini-2.0-flash\",\n",
    "                \"filename\": \"gemini_2_flash_response.md\"\n",
    "            },\n",
    "            \"grok\": {\n",
    "                \"name\": \"Grok 4\",\n",
    "                \"model_id\": \"grok-4\",\n",
    "                \"filename\": \"grok_4_response.md\"\n",
    "            },\n",
    "            \"qwen\": {\n",
    "                \"name\": \"Qwen3-235B\",\n",
    "                \"model_id\": \"qwen3-235b-a22b\",\n",
    "                \"filename\": \"qwen3_235b_response.md\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize clients\n",
    "        self.openai_client = openai.OpenAI(\n",
    "            api_key=self.openai_api_key,\n",
    "            base_url=self.openai_base_url\n",
    "        )\n",
    "\n",
    "config = LLMConfig()\n",
    "print(\"✅ Configuration loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a3458f-bea6-4ec6-bf60-c5508cff5156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Documents loaded successfully!\n",
      "📄 Briefing document: 19307 characters\n",
      "❓ FAQ document: 20092 characters\n",
      "📝 Paper set: Lightweight Dynamic Build Batching Algorithms for Continuous Integration\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: File Reading and Preprocessing\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.briefing_doc = \"\"\n",
    "        self.faq_doc = \"\"\n",
    "        self.paper_title = \"\"\n",
    "        self.paper_url = \"\"\n",
    "    \n",
    "    def load_documents(self, briefing_path: str, faq_path: str):\n",
    "        \"\"\"Load briefing and FAQ documents\"\"\"\n",
    "        try:\n",
    "            with open(briefing_path, 'r', encoding='utf-8') as f:\n",
    "                self.briefing_doc = f.read()\n",
    "            \n",
    "            with open(faq_path, 'r', encoding='utf-8') as f:\n",
    "                self.faq_doc = f.read()\n",
    "            \n",
    "            print(\"✅ Documents loaded successfully!\")\n",
    "            print(f\"📄 Briefing document: {len(self.briefing_doc)} characters\")\n",
    "            print(f\"❓ FAQ document: {len(self.faq_doc)} characters\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"❌ Error loading documents: {e}\")\n",
    "            print(\"Please ensure 'briefing-doc.txt' and 'faq.txt' are in the same directory\")\n",
    "    \n",
    "    def set_paper_info(self, title: str, url: str):\n",
    "        \"\"\"Set paper title and URL\"\"\"\n",
    "        self.paper_title = title\n",
    "        self.paper_url = url\n",
    "        print(f\"📝 Paper set: {title}\")\n",
    "\n",
    "# Initialize document processor\n",
    "doc_processor = DocumentProcessor()\n",
    "\n",
    "# Load documents (you'll need to create these files)\n",
    "doc_processor.load_documents(\"./data/paper-3/briefing-doc.txt\", \"./data/paper-3/faq.txt\")\n",
    "doc_processor.set_paper_info(\"Lightweight Dynamic Build Batching Algorithms for Continuous Integration\", \"https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=XS9QH_UAAAAJ&sortby=pubdate&citation_for_view=XS9QH_UAAAAJ:WAzi4Gm8nLoC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aff493e-964b-4595-9601-b7a5bb0d3b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM clients initialized!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: LLM Client Classes\n",
    "class LLMClient:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.interaction_log = []\n",
    "    \n",
    "    def _log_interaction(self, llm_id: str, task: str, start_time: float, end_time: float, success: bool):\n",
    "        \"\"\"Log LLM interaction\"\"\"\n",
    "        self.interaction_log.append({\n",
    "            'llm_id': llm_id,\n",
    "            'task': task,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': end_time - start_time,\n",
    "            'success': success\n",
    "        })\n",
    "    \n",
    "    def query_gpt(self, prompt: str, task: str = \"general\") -> str:\n",
    "        \"\"\"Query GPT model\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"🔄 Querying {self.config.models['gpt']['name']} for {task}...\")\n",
    "        try:\n",
    "            response = self.config.openai_client.chat.completions.create(\n",
    "                model=self.config.models['gpt']['model_id'],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"gpt\", task, start_time, end_time, True)\n",
    "            print(f\"✅ {self.config.models['gpt']['name']} completed ({len(result)} chars, {end_time-start_time:.2f}s)\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"gpt\", task, start_time, end_time, False)\n",
    "            print(f\"❌ GPT Error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def query_gemini(self, prompt: str, task: str = \"general\") -> str:\n",
    "        \"\"\"Query Gemini model\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"🔄 Querying {self.config.models['gemini']['name']} for {task}...\")\n",
    "        try:\n",
    "            response = self.config.openai_client.chat.completions.create(\n",
    "                model=self.config.models['gemini']['model_id'],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"gemini\", task, start_time, end_time, True)\n",
    "            print(f\"✅ {self.config.models['gemini']['name']} completed ({len(result)} chars, {end_time-start_time:.2f}s)\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"gemini\", task, start_time, end_time, False)\n",
    "            print(f\"❌ Gemini Error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def query_grok(self, prompt: str, task: str = \"general\") -> str:\n",
    "        \"\"\"Query Grok model\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"🔄 Querying {self.config.models['grok']['name']} for {task}...\")\n",
    "        try:\n",
    "            response = self.config.openai_client.chat.completions.create(\n",
    "                model=self.config.models['grok']['model_id'],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"grok\", task, start_time, end_time, True)\n",
    "            print(f\"✅ {self.config.models['grok']['name']} completed ({len(result)} chars, {end_time-start_time:.2f}s)\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"grok\", task, start_time, end_time, False)\n",
    "            print(f\"❌ Grok Error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def query_qwen(self, prompt: str, task: str = \"general\") -> str:\n",
    "        \"\"\"Query Qwen model\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"🔄 Querying {self.config.models['qwen']['name']} for {task}...\")\n",
    "        try:\n",
    "            response = self.config.openai_client.chat.completions.create(\n",
    "                model=self.config.models['qwen']['model_id'],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"qwen\", task, start_time, end_time, True)\n",
    "            print(f\"✅ {self.config.models['qwen']['name']} completed ({len(result)} chars, {end_time-start_time:.2f}s)\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            self._log_interaction(\"qwen\", task, start_time, end_time, False)\n",
    "            print(f\"❌ Qwen Error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "llm_client = LLMClient(config)\n",
    "print(\"✅ LLM clients initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ced728-c6a9-44d1-8a6b-d6b18f3a753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced prompt templates ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Enhanced Prompt Engineering Templates\n",
    "class PromptTemplates:\n",
    "    @staticmethod\n",
    "    def get_critique_prompt(briefing_doc: str, faq_doc: str, paper_title: str) -> str:\n",
    "        return f\"\"\"\n",
    "You are an expert academic reviewer tasked with providing a comprehensive critique of a research paper.\n",
    "\n",
    "**Paper Title:** {paper_title}\n",
    "\n",
    "**Briefing Document:**\n",
    "{briefing_doc}\n",
    "\n",
    "**FAQ Document:**\n",
    "{faq_doc}\n",
    "\n",
    "Based on the provided information, write a concise critique (1-2 pages maximum) with EXACTLY the following structure:\n",
    "\n",
    "## STRENGTHS\n",
    "\n",
    "**Strength 1:** [Title]\n",
    "[Detailed explanation with specific examples from the paper]\n",
    "\n",
    "**Strength 2:** [Title]\n",
    "[Detailed explanation with specific examples from the paper]\n",
    "\n",
    "**Strength 3:** [Title]\n",
    "[Detailed explanation with specific examples from the paper]\n",
    "\n",
    "## LIMITATIONS\n",
    "\n",
    "**Limitation 1:** [Title]\n",
    "[Detailed explanation of the weakness and why it matters]\n",
    "\n",
    "**Limitation 2:** [Title]\n",
    "[Detailed explanation of the weakness and why it matters]\n",
    "\n",
    "**Limitation 3:** [Title]\n",
    "[Detailed explanation of the weakness and why it matters]\n",
    "\n",
    "## RESEARCH_SUGGESTIONS\n",
    "\n",
    "**Suggestion 1:** [Title]\n",
    "[Detailed explanation of the research direction and its value]\n",
    "\n",
    "**Suggestion 2:** [Title]\n",
    "[Detailed explanation of the research direction and its value]\n",
    "\n",
    "**Suggestion 3:** [Title]\n",
    "[Detailed explanation of the research direction and its value]\n",
    "\n",
    "**Requirements:**\n",
    "- Use EXACTLY the format shown above with section headers: STRENGTHS, LIMITATIONS, RESEARCH_SUGGESTIONS\n",
    "- Each item must have a clear title followed by detailed explanation\n",
    "- Be specific and cite concrete examples from the paper\n",
    "- Use academic language but remain clear\n",
    "- Focus on technical contributions and methodology\n",
    "- Ensure suggestions are feasible and well-motivated\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_component_extraction_prompt(responses: List[str], component_type: str) -> str:\n",
    "        responses_text = \"\\n\\n\".join([f\"**Response {i+1}:**\\n{resp}\" for i, resp in enumerate(responses)])\n",
    "        \n",
    "        return f\"\"\"\n",
    "You are an expert academic reviewer. Extract all {component_type.upper()} from the following critique responses.\n",
    "\n",
    "{responses_text}\n",
    "\n",
    "Please extract and list ALL {component_type} mentioned across all responses. For each {component_type.rstrip('s')}, provide:\n",
    "\n",
    "1. **Title:** A concise descriptive title\n",
    "2. **Content:** The full explanation/description\n",
    "3. **Source:** Which response it came from (Response 1, 2, 3, etc.)\n",
    "\n",
    "Format your response as:\n",
    "\n",
    "## EXTRACTED_{component_type.upper()}\n",
    "\n",
    "**Item 1:**\n",
    "- Title: [Title here]\n",
    "- Content: [Full content here]\n",
    "- Source: Response X\n",
    "\n",
    "**Item 2:**\n",
    "- Title: [Title here]\n",
    "- Content: [Full content here]\n",
    "- Source: Response X\n",
    "\n",
    "[Continue for all items found]\n",
    "\n",
    "Be thorough and extract every {component_type.rstrip('s')} mentioned, even if they seem similar.\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_component_ranking_prompt(items: List[Dict], component_type: str, briefing_doc: str, faq_doc: str) -> str:\n",
    "        items_text = \"\\n\\n\".join([\n",
    "            f\"**Item {i+1}:**\\n- Title: {item['title']}\\n- Content: {item['content']}\"\n",
    "            for i, item in enumerate(items)\n",
    "        ])\n",
    "        \n",
    "        return f\"\"\"\n",
    "You are an expert academic reviewer. Below are {component_type} extracted from multiple paper critiques.\n",
    "\n",
    "**Paper Context:**\n",
    "{briefing_doc[:1000]}...\n",
    "\n",
    "**Available {component_type.title()}:**\n",
    "{items_text}\n",
    "\n",
    "Your task:\n",
    "1. **Identify and remove duplicates** - Group similar/duplicate items together\n",
    "2. **Rank the remaining unique items** from best to worst based on:\n",
    "   - Academic rigor and insight\n",
    "   - Specificity and concrete examples  \n",
    "   - Relevance to the paper\n",
    "   - Constructive value\n",
    "   - Technical depth\n",
    "\n",
    "Please respond in this EXACT format:\n",
    "\n",
    "## DUPLICATE_GROUPS\n",
    "Group 1: Items X, Y, Z (explain why they're duplicates)\n",
    "Group 2: Items A, B (explain why they're duplicates)\n",
    "[Continue for all duplicate groups found]\n",
    "\n",
    "## FINAL_RANKING\n",
    "1. Item X: [Brief justification]\n",
    "2. Item Y: [Brief justification]  \n",
    "3. Item Z: [Brief justification]\n",
    "[Continue ranking all UNIQUE items]\n",
    "\n",
    "Select the TOP 3 unique {component_type} that provide the most value for an academic paper critique.\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_final_synthesis_prompt(top_strengths: List[Dict], top_limitations: List[Dict], \n",
    "                                 top_suggestions: List[Dict], paper_title: str) -> str:\n",
    "        \n",
    "        strengths_text = \"\\n\".join([f\"**{s['title']}:** {s['content']}\" for s in top_strengths])\n",
    "        limitations_text = \"\\n\".join([f\"**{l['title']}:** {l['content']}\" for l in top_limitations])\n",
    "        suggestions_text = \"\\n\".join([f\"**{s['title']}:** {s['content']}\" for s in top_suggestions])\n",
    "        \n",
    "        return f\"\"\"\n",
    "You are an expert academic writer. Create a well-structured paper critique using the following top-ranked components:\n",
    "\n",
    "**Paper Title:** {paper_title}\n",
    "\n",
    "**TOP STRENGTHS:**\n",
    "{strengths_text}\n",
    "\n",
    "**TOP LIMITATIONS:**\n",
    "{limitations_text}\n",
    "\n",
    "**TOP RESEARCH SUGGESTIONS:**\n",
    "{suggestions_text}\n",
    "\n",
    "Create a professional academic critique that:\n",
    "\n",
    "1. **Integrates these components**\n",
    "2. **Maintains academic rigor** and professional tone\n",
    "3. **Ensures logical organization**\n",
    "4. **Eliminates any redundancy** while preserving all key insights\n",
    "5. **Follows standard academic critique format**\n",
    "\n",
    "Structure your response as:\n",
    "\n",
    "# Paper Critique\n",
    "\n",
    "## Three Strengths\n",
    "\n",
    "[Put the 3 top strengths here]\n",
    "\n",
    "## Three Limitations\n",
    "\n",
    "[Put the 3 top limitations here]\n",
    "\n",
    "## Three Research Suggestions\n",
    "\n",
    "[Put the 3 top suggestions here]\n",
    "\n",
    "The final critique should be 1-2 pages, academically rigorous, and ready for submission.\n",
    "\"\"\"\n",
    "\n",
    "prompt_templates = PromptTemplates()\n",
    "print(\"✅ Enhanced prompt templates ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4839d8b5-e0e7-41cd-ab02-a88a35c4cdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Component analyzer initialized!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Component Analysis System\n",
    "class ComponentAnalyzer:\n",
    "    def __init__(self, llm_client):\n",
    "        self.llm_client = llm_client\n",
    "        self.extracted_components = {\n",
    "            'strengths': [],\n",
    "            'limitations': [],\n",
    "            'suggestions': []\n",
    "        }\n",
    "        self.ranked_components = {\n",
    "            'strengths': [],\n",
    "            'limitations': [], \n",
    "            'suggestions': []\n",
    "        }\n",
    "        self.top_components = {\n",
    "            'strengths': [],\n",
    "            'limitations': [],\n",
    "            'suggestions': []\n",
    "        }\n",
    "    \n",
    "    def extract_components_from_responses(self, responses: Dict[str, str]) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Extract strengths, limitations, and suggestions from all responses\"\"\"\n",
    "        print(\"🔍 Extracting components from individual responses...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Filter valid responses\n",
    "        valid_responses = [resp for resp in responses.values() if not resp.startswith(\"Error\")]\n",
    "        \n",
    "        if len(valid_responses) < 2:\n",
    "            print(\"❌ Not enough valid responses for component extraction\")\n",
    "            return self.extracted_components\n",
    "        \n",
    "        component_types = ['strengths', 'limitations', 'suggestions']\n",
    "        \n",
    "        for component_type in component_types:\n",
    "            print(f\"\\n📋 Extracting {component_type}...\")\n",
    "            \n",
    "            # Get extraction prompt\n",
    "            extraction_prompt = prompt_templates.get_component_extraction_prompt(\n",
    "                valid_responses, component_type\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Use GPT for extraction\n",
    "                extraction_result = self.llm_client.query_gpt(\n",
    "                    extraction_prompt, f\"extract_{component_type}\"\n",
    "                )\n",
    "                \n",
    "                # Parse extraction result\n",
    "                components = self._parse_extraction_result(extraction_result, component_type)\n",
    "                self.extracted_components[component_type] = components\n",
    "                \n",
    "                # Save extraction results\n",
    "                self._save_extraction_results(component_type, extraction_result, components)\n",
    "                \n",
    "                print(f\"✅ Extracted {len(components)} {component_type}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to extract {component_type}: {e}\")\n",
    "        \n",
    "        return self.extracted_components\n",
    "    \n",
    "    def _parse_extraction_result(self, extraction_text: str, component_type: str) -> List[Dict]:\n",
    "        \"\"\"Parse the extraction result into structured components\"\"\"\n",
    "        components = []\n",
    "        lines = extraction_text.split('\\n')\n",
    "        \n",
    "        current_item = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('**Item ') and line.endswith(':**'):\n",
    "                if current_item:\n",
    "                    components.append(current_item)\n",
    "                current_item = {'title': '', 'content': '', 'source': ''}\n",
    "            \n",
    "            elif current_item is not None:\n",
    "                if line.startswith('- Title:'):\n",
    "                    current_item['title'] = line.replace('- Title:', '').strip()\n",
    "                elif line.startswith('- Content:'):\n",
    "                    current_item['content'] = line.replace('- Content:', '').strip()\n",
    "                elif line.startswith('- Source:'):\n",
    "                    current_item['source'] = line.replace('- Source:', '').strip()\n",
    "                elif line and not line.startswith('-') and current_item['content']:\n",
    "                    # Continue content on next line\n",
    "                    current_item['content'] += ' ' + line\n",
    "        \n",
    "        if current_item:\n",
    "            components.append(current_item)\n",
    "        \n",
    "        return components\n",
    "    \n",
    "    def rank_components(self, briefing_doc: str, faq_doc: str) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Rank components using all LLMs and find consensus\"\"\"\n",
    "        print(\"\\n🏆 Starting component ranking phase...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        llm_methods = {\n",
    "            'gpt': self.llm_client.query_gpt,\n",
    "            'gemini': self.llm_client.query_gemini,\n",
    "            'grok': self.llm_client.query_grok,\n",
    "            'qwen': self.llm_client.query_qwen\n",
    "        }\n",
    "        \n",
    "        for component_type in ['strengths', 'limitations', 'suggestions']:\n",
    "            if not self.extracted_components[component_type]:\n",
    "                print(f\"⚠️  No {component_type} to rank\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n📊 Ranking {component_type}...\")\n",
    "            \n",
    "            # Randomize order for unbiased ranking\n",
    "            items = self.extracted_components[component_type].copy()\n",
    "            random.shuffle(items)\n",
    "            \n",
    "            ranking_prompt = prompt_templates.get_component_ranking_prompt(\n",
    "                items, component_type, briefing_doc, faq_doc\n",
    "            )\n",
    "            \n",
    "            # Get rankings from all LLMs\n",
    "            all_rankings = {}\n",
    "            for llm_id, method in llm_methods.items():\n",
    "                try:\n",
    "                    print(f\"🎯 Getting {component_type} rankings from {config.models[llm_id]['name']}...\")\n",
    "                    ranking_result = method(ranking_prompt, f\"rank_{component_type}\")\n",
    "                    \n",
    "                    # Parse ranking result\n",
    "                    parsed_ranking = self._parse_ranking_result(ranking_result, items)\n",
    "                    all_rankings[llm_id] = parsed_ranking\n",
    "                    \n",
    "                    print(f\"✅ Rankings received from {config.models[llm_id]['name']}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Ranking failed for {config.models[llm_id]['name']}: {e}\")\n",
    "            \n",
    "            # Calculate consensus ranking\n",
    "            if all_rankings:\n",
    "                consensus_ranking = self._calculate_consensus_ranking(all_rankings, items)\n",
    "                self.ranked_components[component_type] = consensus_ranking\n",
    "                \n",
    "                # Save ranking results\n",
    "                self._save_ranking_results(component_type, all_rankings, consensus_ranking)\n",
    "                \n",
    "                # Get top 3 unique components\n",
    "                self.top_components[component_type] = consensus_ranking[:3]\n",
    "                \n",
    "                print(f\"🏅 Top 3 {component_type}:\")\n",
    "                for i, item in enumerate(self.top_components[component_type]):\n",
    "                    print(f\"  {i+1}. {item['title']}\")\n",
    "        \n",
    "        return self.top_components\n",
    "    \n",
    "    def _parse_ranking_result(self, ranking_text: str, items: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Parse ranking result from LLM\"\"\"\n",
    "        # Simple parsing - extract item numbers from ranking\n",
    "        import re\n",
    "        \n",
    "        # Find the FINAL_RANKING section\n",
    "        ranking_section = \"\"\n",
    "        if \"FINAL_RANKING\" in ranking_text:\n",
    "            ranking_section = ranking_text.split(\"FINAL_RANKING\")[1]\n",
    "        else:\n",
    "            ranking_section = ranking_text\n",
    "        \n",
    "        # Extract ranked order\n",
    "        ranked_items = []\n",
    "        lines = ranking_section.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # Look for numbered items like \"1. Item X\" or \"1. Item 3\"\n",
    "            match = re.match(r'(\\d+)\\.\\s*Item\\s*(\\d+)', line)\n",
    "            if match:\n",
    "                item_index = int(match.group(2)) - 1  # Convert to 0-based index\n",
    "                if 0 <= item_index < len(items):\n",
    "                    ranked_items.append(items[item_index])\n",
    "        \n",
    "        # If parsing failed, return original order\n",
    "        if not ranked_items:\n",
    "            return items\n",
    "        \n",
    "        # Add any missing items at the end\n",
    "        for item in items:\n",
    "            if item not in ranked_items:\n",
    "                ranked_items.append(item)\n",
    "        \n",
    "        return ranked_items\n",
    "    \n",
    "    def _calculate_consensus_ranking(self, all_rankings: Dict[str, List[Dict]], items: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Calculate consensus ranking using Borda count\"\"\"\n",
    "        scores = {i: 0 for i in range(len(items))}\n",
    "        \n",
    "        # Calculate Borda scores\n",
    "        for llm_id, ranking in all_rankings.items():\n",
    "            for pos, item in enumerate(ranking):\n",
    "                # Find item index in original list\n",
    "                for i, original_item in enumerate(items):\n",
    "                    if original_item['title'] == item['title']:\n",
    "                        scores[i] += len(items) - pos\n",
    "                        break\n",
    "        \n",
    "        # Sort by score (highest first)\n",
    "        ranked_indices = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n",
    "        consensus_ranking = [items[i] for i in ranked_indices]\n",
    "        \n",
    "        return consensus_ranking\n",
    "    \n",
    "    def _save_extraction_results(self, component_type: str, raw_result: str, parsed_components: List[Dict]):\n",
    "        \"\"\"Save extraction results to files\"\"\"\n",
    "        # Save raw result\n",
    "        with open(f'./results/component_analysis/{component_type}_extraction_raw.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# {component_type.title()} Extraction Results\\n\\n\")\n",
    "            f.write(raw_result)\n",
    "        \n",
    "        # Save parsed components\n",
    "        with open(f'./results/component_analysis/{component_type}_extracted.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(parsed_components, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def _save_ranking_results(self, component_type: str, all_rankings: Dict, consensus: List[Dict]):\n",
    "        \"\"\"Save ranking results to files\"\"\"\n",
    "        # Save all rankings\n",
    "        with open(f'./results/component_analysis/{component_type}_rankings.json', 'w', encoding='utf-8') as f:\n",
    "            # Convert to serializable format\n",
    "            serializable_rankings = {}\n",
    "            for llm_id, ranking in all_rankings.items():\n",
    "                serializable_rankings[llm_id] = [item['title'] for item in ranking]\n",
    "            json.dump(serializable_rankings, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save consensus ranking\n",
    "        with open(f'./results/component_analysis/{component_type}_consensus.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(consensus, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "component_analyzer = ComponentAnalyzer(llm_client)\n",
    "print(\"✅ Component analyzer initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f7caf4-c6cd-4154-ab88-3d9f06b7cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced multi-agent review system initialized!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Enhanced Multi-Agent Review System\n",
    "class MultiAgentReviewSystem:\n",
    "    def __init__(self, llm_client, doc_processor, component_analyzer):\n",
    "        self.llm_client = llm_client\n",
    "        self.doc_processor = doc_processor\n",
    "        self.component_analyzer = component_analyzer\n",
    "        self.responses = {}\n",
    "        self.final_synthesis = \"\"\n",
    "    \n",
    "    def generate_individual_responses(self) -> Dict[str, str]:\n",
    "        \"\"\"Generate critiques from all LLM models and save them\"\"\"\n",
    "        print(\"🚀 Generating individual responses from all LLMs...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        base_prompt = prompt_templates.get_critique_prompt(\n",
    "            self.doc_processor.briefing_doc,\n",
    "            self.doc_processor.faq_doc,\n",
    "            self.doc_processor.paper_title\n",
    "        )\n",
    "        \n",
    "        # Query each LLM\n",
    "        llm_methods = {\n",
    "            'gpt': self.llm_client.query_gpt,\n",
    "            'gemini': self.llm_client.query_gemini,\n",
    "            'grok': self.llm_client.query_grok,\n",
    "            'qwen': self.llm_client.query_qwen\n",
    "        }\n",
    "        \n",
    "        for llm_id, method in llm_methods.items():            \n",
    "            try:\n",
    "                response = method(base_prompt, \"critique_generation\")\n",
    "                self.responses[llm_id] = response\n",
    "                \n",
    "                # Save individual response\n",
    "                filename = config.models[llm_id]['filename']\n",
    "                filepath = f\"./results/individual_responses/{filename}\"\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"# {config.models[llm_id]['name']} Response\\n\")\n",
    "                    f.write(f\"**Paper:** {self.doc_processor.paper_title}\\n\")\n",
    "                    f.write(f\"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "                    f.write(response)\n",
    "                \n",
    "                print(f\"💾 Saved response to: {filepath}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ {config.models[llm_id]['name']} failed: {e}\")\n",
    "                self.responses[llm_id] = f\"Error generating response: {e}\"\n",
    "            \n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        print(f\"📊 Summary: {len([r for r in self.responses.values() if not r.startswith('Error')])} successful responses out of {len(self.responses)}\")\n",
    "        return self.responses\n",
    "    \n",
    "    def analyze_and_rank_components(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Extract and rank components from responses\"\"\"\n",
    "        print(\"\\n🔬 Starting component analysis phase...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Extract components\n",
    "        extracted = self.component_analyzer.extract_components_from_responses(self.responses)\n",
    "        \n",
    "        # Step 2: Rank components\n",
    "        top_components = self.component_analyzer.rank_components(\n",
    "            self.doc_processor.briefing_doc,\n",
    "            self.doc_processor.faq_doc\n",
    "        )\n",
    "        \n",
    "        return top_components\n",
    "    \n",
    "    def synthesize_final_response(self, top_components: Dict[str, List[Dict]]) -> str:\n",
    "        \"\"\"Create final synthesized response using top-ranked components\"\"\"\n",
    "        print(\"\\n🔄 Synthesizing final response from top components...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Prepare synthesis prompt with top components\n",
    "        synthesis_prompt = prompt_templates.get_final_synthesis_prompt(\n",
    "            top_components.get('strengths', []),\n",
    "            top_components.get('limitations', []),\n",
    "            top_components.get('suggestions', []),\n",
    "            self.doc_processor.paper_title\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            self.final_synthesis = self.llm_client.query_qwen(synthesis_prompt, \"final_synthesis\")\n",
    "            print(\"✅ Final synthesis completed!\")\n",
    "            \n",
    "            # Save final synthesis\n",
    "            with open('./results/final_critique.md', 'w', encoding='utf-8') as f:\n",
    "                f.write(self.final_synthesis)\n",
    "            print(\"💾 Final critique saved to: ./results/final_critique.md\")\n",
    "            \n",
    "            # Save synthesis details\n",
    "            self._save_synthesis_details(top_components)\n",
    "            \n",
    "            return self.final_synthesis\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Synthesis failed: {e}\")\n",
    "            return f\"Error in synthesis: {e}\"\n",
    "    \n",
    "    def _save_synthesis_details(self, top_components: Dict[str, List[Dict]]):\n",
    "        \"\"\"Save details about the synthesis process\"\"\"\n",
    "        synthesis_details = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'paper_title': self.doc_processor.paper_title,\n",
    "            'top_components_used': top_components,\n",
    "            'final_synthesis_length': len(self.final_synthesis.split()) if self.final_synthesis else 0\n",
    "        }\n",
    "        \n",
    "        with open('./results/component_analysis/synthesis_details.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(synthesis_details, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "review_system = MultiAgentReviewSystem(llm_client, doc_processor, component_analyzer)\n",
    "print(\"✅ Enhanced multi-agent review system initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa0d082-3e83-467e-a7d8-f4919a1630e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced results analyzer ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Enhanced Results Analysis\n",
    "class ResultsAnalyzer:\n",
    "    def __init__(self, review_system, llm_client):\n",
    "        self.review_system = review_system\n",
    "        self.llm_client = llm_client\n",
    "    \n",
    "    def create_response_comparison_table(self) -> pd.DataFrame:\n",
    "        \"\"\"Create comparison table of all responses\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for llm_id, response in self.review_system.responses.items():\n",
    "            word_count = len(response.split())\n",
    "            char_count = len(response)\n",
    "            error_status = \"Error\" if response.startswith(\"Error\") else \"Success\"\n",
    "            \n",
    "            data.append({\n",
    "                'LLM': config.models[llm_id]['name'],\n",
    "                'Status': error_status,\n",
    "                'Word Count': word_count,\n",
    "                'Character Count': char_count,\n",
    "                'Response Preview': response[:200] + \"...\" if len(response) > 200 else response\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def create_component_analysis_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create summary of component analysis\"\"\"\n",
    "        summary = {\n",
    "            'extraction_stats': {},\n",
    "            'ranking_stats': {},\n",
    "            'final_selection': {}\n",
    "        }\n",
    "        \n",
    "        for component_type in ['strengths', 'limitations', 'suggestions']:\n",
    "            # Extraction stats\n",
    "            extracted = self.review_system.component_analyzer.extracted_components.get(component_type, [])\n",
    "            summary['extraction_stats'][component_type] = {\n",
    "                'total_extracted': len(extracted),\n",
    "                'unique_titles': len(set(item['title'] for item in extracted))\n",
    "            }\n",
    "            \n",
    "            # Final selection\n",
    "            top_items = self.review_system.component_analyzer.top_components.get(component_type, [])\n",
    "            summary['final_selection'][component_type] = [\n",
    "                {'title': item['title'], 'content_preview': item['content'][:100] + '...'}\n",
    "                for item in top_items\n",
    "            ]\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def create_synthesis_quality_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate quality metrics for the final synthesis\"\"\"\n",
    "        try:\n",
    "            synthesis = self.review_system.final_synthesis\n",
    "            \n",
    "            if not synthesis or synthesis.startswith(\"Error\"):\n",
    "                print(\"⚠️  No valid synthesis available for metrics\")\n",
    "                return {}\n",
    "            \n",
    "            metrics = {\n",
    "                'Word Count': len(synthesis.split()),\n",
    "                'Character Count': len(synthesis),\n",
    "                'Paragraph Count': len([p for p in synthesis.split('\\n\\n') if p.strip()]),\n",
    "                'Sentence Count': len([s for s in synthesis.split('.') if s.strip()]),\n",
    "                'Average Sentence Length': len(synthesis.split()) / max(len([s for s in synthesis.split('.') if s.strip()]), 1)\n",
    "            }\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Metrics calculation error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def generate_final_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive final report\"\"\"\n",
    "        try:\n",
    "            component_summary = self.create_component_analysis_summary()\n",
    "            \n",
    "            report = f\"\"\"\n",
    "# Enhanced Paper Review Analysis Report\n",
    "\n",
    "## Paper Information\n",
    "- **Title:** {self.review_system.doc_processor.paper_title}\n",
    "- **URL:** {self.review_system.doc_processor.paper_url}\n",
    "\n",
    "## Process Summary\n",
    "- **LLMs Used:** {len(self.review_system.responses)} models\n",
    "- **Successful Responses:** {sum(1 for r in self.review_system.responses.values() if not r.startswith('Error'))}\n",
    "- **Component Extraction:** Completed for strengths, limitations, suggestions\n",
    "- **Component Ranking:** Multi-LLM consensus ranking applied\n",
    "\n",
    "## Component Analysis Results\n",
    "\"\"\"\n",
    "            \n",
    "            for component_type in ['strengths', 'limitations', 'suggestions']:\n",
    "                stats = component_summary['extraction_stats'].get(component_type, {})\n",
    "                report += f\"\"\"\n",
    "### {component_type.title()}\n",
    "- **Total Extracted:** {stats.get('total_extracted', 0)}\n",
    "- **Unique Items:** {stats.get('unique_titles', 0)}\n",
    "- **Top 3 Selected:** {len(component_summary['final_selection'].get(component_type, []))}\n",
    "\"\"\"\n",
    "            \n",
    "            report += \"\\n## Individual Response Files\\n\"\n",
    "            \n",
    "            for llm_id in self.review_system.responses.keys():\n",
    "                filename = config.models[llm_id]['filename']\n",
    "                report += f\"- **{config.models[llm_id]['name']}:** ./results/individual_responses/{filename}\\n\"\n",
    "            \n",
    "            report += \"\\n## Component Analysis Files\\n\"\n",
    "            component_files = [\n",
    "                \"strengths_extraction_raw.md\", \"limitations_extraction_raw.md\", \"suggestions_extraction_raw.md\",\n",
    "                \"strengths_extracted.json\", \"limitations_extracted.json\", \"suggestions_extracted.json\",\n",
    "                \"strengths_rankings.json\", \"limitations_rankings.json\", \"suggestions_rankings.json\",\n",
    "                \"strengths_consensus.json\", \"limitations_consensus.json\", \"suggestions_consensus.json\",\n",
    "                \"synthesis_details.json\"\n",
    "            ]\n",
    "            \n",
    "            for file in component_files:\n",
    "                report += f\"- **{file}:** ./results/component_analysis/{file}\\n\"\n",
    "            \n",
    "            report += \"\\n## Quality Metrics\\n\"\n",
    "            \n",
    "            metrics = self.create_synthesis_quality_metrics()\n",
    "            for metric, value in metrics.items():\n",
    "                report += f\"- **{metric}:** {value}\\n\"\n",
    "            \n",
    "            report += f\"\"\"\n",
    "## Interaction Log Summary\n",
    "- **Total Interactions:** {len(self.llm_client.interaction_log)}\n",
    "- **Successful Interactions:** {sum(1 for log in self.llm_client.interaction_log if log['success'])}\n",
    "- **Total Processing Time:** {sum(log['duration'] for log in self.llm_client.interaction_log):.2f} seconds\n",
    "\n",
    "## Final Synthesized Response\n",
    "\n",
    "{self.review_system.final_synthesis}\n",
    "\n",
    "---\n",
    "*Generated by Enhanced Multi-Agent Academic Review System with Component-Based Analysis*\n",
    "\"\"\"\n",
    "            \n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating report: {e}\"\n",
    "\n",
    "results_analyzer = ResultsAnalyzer(review_system, llm_client)\n",
    "print(\"✅ Enhanced results analyzer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb6f5a6-d422-425f-8c59-eca95058aa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to run the enhanced pipeline!\n",
      "Make sure you have:\n",
      "1. ✅ briefing-doc.txt file in the data directory\n",
      "2. ✅ faq.txt file in the data directory\n",
      "3. ✅ API keys configured\n",
      "\n",
      "Run the next cell to start the enhanced pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Enhanced Main Execution Pipeline\n",
    "def run_complete_pipeline():\n",
    "    \"\"\"Execute the complete enhanced paper review pipeline\"\"\"\n",
    "    print(\"🚀 Starting Enhanced Paper Review Pipeline\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Generate individual responses\n",
    "    print(\"\\n📝 STEP 1: Generating Individual Responses\")\n",
    "    responses = review_system.generate_individual_responses()\n",
    "    \n",
    "    # Display response comparison\n",
    "    comparison_df = results_analyzer.create_response_comparison_table()\n",
    "    print(\"\\n📊 Response Comparison Summary:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Step 2: Extract and rank components\n",
    "    print(\"\\n🔬 STEP 2: Component Analysis and Ranking\")\n",
    "    top_components = review_system.analyze_and_rank_components()\n",
    "    \n",
    "    # Display component analysis summary\n",
    "    component_summary = results_analyzer.create_component_analysis_summary()\n",
    "    print(\"\\n📋 Component Analysis Summary:\")\n",
    "    for component_type, stats in component_summary['extraction_stats'].items():\n",
    "        print(f\"  {component_type.title()}: {stats['total_extracted']} extracted → 3 selected\")\n",
    "    \n",
    "    # Step 3: Create final synthesis\n",
    "    print(\"\\n🔄 STEP 3: Creating Final Synthesis from Top Components\")\n",
    "    final_response = review_system.synthesize_final_response(top_components)\n",
    "    \n",
    "    # Step 4: Generate final report\n",
    "    print(\"\\n📋 STEP 4: Generating Final Report\")\n",
    "    final_report = results_analyzer.generate_final_report()\n",
    "    \n",
    "    # Save final report\n",
    "    with open('./results/full_report.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(final_report)\n",
    "    \n",
    "    print(\"\\n✅ Enhanced pipeline completed! Files saved:\")\n",
    "    print(\"   📄 ./results/final_critique.md (Main deliverable)\")\n",
    "    print(\"   📋 ./results/full_report.md (Complete analysis)\")\n",
    "    print(\"   📁 ./results/individual_responses/ (Individual LLM responses)\")\n",
    "    print(\"   📁 ./results/component_analysis/ (Component extraction & ranking)\")\n",
    "    \n",
    "    return final_response, final_report\n",
    "\n",
    "# Execute the pipeline\n",
    "print(\"Ready to run the enhanced pipeline!\")\n",
    "print(\"Make sure you have:\")\n",
    "print(\"1. ✅ briefing-doc.txt file in the data directory\")\n",
    "print(\"2. ✅ faq.txt file in the data directory\") \n",
    "print(\"3. ✅ API keys configured\")\n",
    "print(\"\\nRun the next cell to start the enhanced pipeline!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a8142c-6887-491c-8eff-b4e02e1070dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Enhanced Paper Review Pipeline\n",
      "================================================================================\n",
      "\n",
      "📝 STEP 1: Generating Individual Responses\n",
      "🚀 Generating individual responses from all LLMs...\n",
      "============================================================\n",
      "🔄 Querying o4-mini for critique_generation...\n",
      "✅ o4-mini completed (4561 chars, 22.14s)\n",
      "💾 Saved response to: ./results/individual_responses/o4_mini_response.md\n",
      "----------------------------------------\n",
      "🔄 Querying Gemini 2.0 Flash for critique_generation...\n",
      "✅ Gemini 2.0 Flash completed (8028 chars, 10.46s)\n",
      "💾 Saved response to: ./results/individual_responses/gemini_2_flash_response.md\n",
      "----------------------------------------\n",
      "🔄 Querying Grok 4 for critique_generation...\n",
      "✅ Grok 4 completed (5394 chars, 32.66s)\n",
      "💾 Saved response to: ./results/individual_responses/grok_4_response.md\n",
      "----------------------------------------\n",
      "🔄 Querying Qwen3-235B for critique_generation...\n",
      "✅ Qwen3-235B completed (8392 chars, 71.23s)\n",
      "💾 Saved response to: ./results/individual_responses/qwen3_235b_response.md\n",
      "----------------------------------------\n",
      "📊 Summary: 4 successful responses out of 4\n",
      "\n",
      "📊 Response Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM</th>\n",
       "      <th>Status</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Character Count</th>\n",
       "      <th>Response Preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>Success</td>\n",
       "      <td>598</td>\n",
       "      <td>4561</td>\n",
       "      <td>## STRENGTHS\\n\\n**Strength 1:** Simplicity and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gemini 2.0 Flash</td>\n",
       "      <td>Success</td>\n",
       "      <td>1099</td>\n",
       "      <td>8028</td>\n",
       "      <td>## STRENGTHS\\n\\n**Strength 1: Simplicity and F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grok 4</td>\n",
       "      <td>Success</td>\n",
       "      <td>708</td>\n",
       "      <td>5394</td>\n",
       "      <td>## STRENGTHS\\n\\n**Strength 1:** Innovative Sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen3-235B</td>\n",
       "      <td>Success</td>\n",
       "      <td>1102</td>\n",
       "      <td>8392</td>\n",
       "      <td>## STRENGTHS  \\n\\n**Strength 1: Lightweight On...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                LLM   Status  Word Count  Character Count  \\\n",
       "0           o4-mini  Success         598             4561   \n",
       "1  Gemini 2.0 Flash  Success        1099             8028   \n",
       "2            Grok 4  Success         708             5394   \n",
       "3        Qwen3-235B  Success        1102             8392   \n",
       "\n",
       "                                    Response Preview  \n",
       "0  ## STRENGTHS\\n\\n**Strength 1:** Simplicity and...  \n",
       "1  ## STRENGTHS\\n\\n**Strength 1: Simplicity and F...  \n",
       "2  ## STRENGTHS\\n\\n**Strength 1:** Innovative Sim...  \n",
       "3  ## STRENGTHS  \\n\\n**Strength 1: Lightweight On...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 STEP 2: Component Analysis and Ranking\n",
      "\n",
      "🔬 Starting component analysis phase...\n",
      "============================================================\n",
      "🔍 Extracting components from individual responses...\n",
      "============================================================\n",
      "\n",
      "📋 Extracting strengths...\n",
      "🔄 Querying o4-mini for extract_strengths...\n",
      "✅ o4-mini completed (11021 chars, 44.16s)\n",
      "✅ Extracted 12 strengths\n",
      "\n",
      "📋 Extracting limitations...\n",
      "🔄 Querying o4-mini for extract_limitations...\n",
      "✅ o4-mini completed (8408 chars, 26.52s)\n",
      "✅ Extracted 12 limitations\n",
      "\n",
      "📋 Extracting suggestions...\n",
      "🔄 Querying o4-mini for extract_suggestions...\n",
      "✅ o4-mini completed (7968 chars, 21.42s)\n",
      "✅ Extracted 12 suggestions\n",
      "\n",
      "🏆 Starting component ranking phase...\n",
      "============================================================\n",
      "\n",
      "📊 Ranking strengths...\n",
      "🎯 Getting strengths rankings from o4-mini...\n",
      "🔄 Querying o4-mini for rank_strengths...\n",
      "✅ o4-mini completed (2163 chars, 21.01s)\n",
      "✅ Rankings received from o4-mini\n",
      "🎯 Getting strengths rankings from Gemini 2.0 Flash...\n",
      "🔄 Querying Gemini 2.0 Flash for rank_strengths...\n",
      "✅ Gemini 2.0 Flash completed (3904 chars, 6.39s)\n",
      "✅ Rankings received from Gemini 2.0 Flash\n",
      "🎯 Getting strengths rankings from Grok 4...\n",
      "🔄 Querying Grok 4 for rank_strengths...\n",
      "❌ Grok Error: Connection error.\n",
      "✅ Rankings received from Grok 4\n",
      "🎯 Getting strengths rankings from Qwen3-235B...\n",
      "🔄 Querying Qwen3-235B for rank_strengths...\n",
      "❌ Qwen Error: Connection error.\n",
      "✅ Rankings received from Qwen3-235B\n",
      "🏅 Top 3 strengths:\n",
      "  1. Robust Empirical Evaluation of Batching Trade-offs\n",
      "  2. Customizable and Explainable Batching Policies\n",
      "  3. Innovative Simplicity in Dynamic Batching\n",
      "\n",
      "📊 Ranking limitations...\n",
      "🎯 Getting limitations rankings from o4-mini...\n",
      "🔄 Querying o4-mini for rank_limitations...\n",
      "✅ o4-mini completed (1866 chars, 15.25s)\n",
      "✅ Rankings received from o4-mini\n",
      "🎯 Getting limitations rankings from Gemini 2.0 Flash...\n",
      "🔄 Querying Gemini 2.0 Flash for rank_limitations...\n",
      "✅ Gemini 2.0 Flash completed (2677 chars, 4.81s)\n",
      "✅ Rankings received from Gemini 2.0 Flash\n",
      "🎯 Getting limitations rankings from Grok 4...\n",
      "🔄 Querying Grok 4 for rank_limitations...\n",
      "✅ Grok 4 completed (2372 chars, 32.66s)\n",
      "✅ Rankings received from Grok 4\n",
      "🎯 Getting limitations rankings from Qwen3-235B...\n",
      "🔄 Querying Qwen3-235B for rank_limitations...\n",
      "✅ Qwen3-235B completed (4608 chars, 21.64s)\n",
      "✅ Rankings received from Qwen3-235B\n",
      "🏅 Top 3 limitations:\n",
      "  1. Over-Reliance on “Builds Saved” as a Proxy Metric\n",
      "  2. Narrow Dataset and Tooling Scope\n",
      "  3. CI-Skip Integration Offers Marginal Gains\n",
      "\n",
      "📊 Ranking suggestions...\n",
      "🎯 Getting suggestions rankings from o4-mini...\n",
      "🔄 Querying o4-mini for rank_suggestions...\n",
      "✅ o4-mini completed (1992 chars, 16.17s)\n",
      "✅ Rankings received from o4-mini\n",
      "🎯 Getting suggestions rankings from Gemini 2.0 Flash...\n",
      "🔄 Querying Gemini 2.0 Flash for rank_suggestions...\n",
      "✅ Gemini 2.0 Flash completed (2707 chars, 4.58s)\n",
      "✅ Rankings received from Gemini 2.0 Flash\n",
      "🎯 Getting suggestions rankings from Grok 4...\n",
      "🔄 Querying Grok 4 for rank_suggestions...\n",
      "✅ Grok 4 completed (1990 chars, 95.52s)\n",
      "✅ Rankings received from Grok 4\n",
      "🎯 Getting suggestions rankings from Qwen3-235B...\n",
      "🔄 Querying Qwen3-235B for rank_suggestions...\n",
      "✅ Qwen3-235B completed (4793 chars, 23.66s)\n",
      "✅ Rankings received from Qwen3-235B\n",
      "🏅 Top 3 suggestions:\n",
      "  1. Dynamic Optimization of LWD’s Configurable Parameters\n",
      "  2. Integrated Prioritization of Build-Time and Build-Count Optimization\n",
      "  3. Cross-Language and Cross-Tool Validation of LWD\n",
      "\n",
      "📋 Component Analysis Summary:\n",
      "  Strengths: 12 extracted → 3 selected\n",
      "  Limitations: 12 extracted → 3 selected\n",
      "  Suggestions: 12 extracted → 3 selected\n",
      "\n",
      "🔄 STEP 3: Creating Final Synthesis from Top Components\n",
      "\n",
      "🔄 Synthesizing final response from top components...\n",
      "============================================================\n",
      "🔄 Querying Qwen3-235B for final_synthesis...\n",
      "✅ Qwen3-235B completed (7823 chars, 31.95s)\n",
      "✅ Final synthesis completed!\n",
      "💾 Final critique saved to: ./results/final_critique.md\n",
      "\n",
      "📋 STEP 4: Generating Final Report\n",
      "\n",
      "✅ Enhanced pipeline completed! Files saved:\n",
      "   📄 ./results/final_critique.md (Main deliverable)\n",
      "   📋 ./results/full_report.md (Complete analysis)\n",
      "   📁 ./results/individual_responses/ (Individual LLM responses)\n",
      "   📁 ./results/component_analysis/ (Component extraction & ranking)\n",
      "\n",
      "================================================================================\n",
      "🎉 ENHANCED PIPELINE RESULTS\n",
      "================================================================================\n",
      "\n",
      "📝 FINAL CRITIQUE (for Professor Adams):\n",
      "--------------------------------------------------\n",
      "# Paper Critique\n",
      "\n",
      "## Three Strengths\n",
      "\n",
      "**Robust Empirical Evaluation of Batching Trade-offs**  \n",
      "The paper presents a comprehensive empirical evaluation of the proposed Lightweight Dynamic Batching (LWD) algorithm, offering significant contributions through rigorous statistical analysis. The authors evaluate 39 sub-variants of LWD against both static and dynamic batching baselines, employing non-parametric tests (Wilcoxon, Friedman) and effect size measures (Cliff’s Delta, Kendall’s W) to substantiate their findings. Notably, the Linear-4 BatchStop4 variant outperformed 41 of 50 static batching configurations with large effect sizes, demonstrating LWD’s consistent superiority in reducing build counts. Moreover, the study disentangles the interaction between batching strategies and fallback algorithms, revealing that Linear variants perform best with BatchStop4 while Exponential variants excel with BatchDivide4. These insights are actionable and valuable for practitioners seeking to optimize CI pipelines. The evaluation also spans a wide range of project failure rates (5.84% to 98.32%), reinforcing the algorithm’s robustness across diverse development environments.\n",
      "\n",
      "**Customizable and Explainable Batching Policies**  \n",
      "A notable strength of LWD lies in its transparent and customizable design. Engineers can adjust parameters such as batch size limits, fallback thresholds (e.g., 40% failure rate), and update rules (e.g., linear or exponential scaling), enabling fine-grained adaptation to project-specific dynamics. For instance, the Most-Frequent-Used (MFU) variant adapts to historical batch size usage during high-failure periods, offering a data-informed yet interpretable mechanism. Unlike black-box approaches, LWD’s arithmetic rules (e.g., the “Factor Rule”) are straightforward, facilitating debugging and refinement without reliance on complex models. This simplicity is contrasted effectively with CI-Skip rules, which, while similarly interpretable, are shown to be less effective when integrated with batching strategies. The inclusion of an online replication package further enhances the practical value of the work by ensuring reproducibility and ease of adoption.\n",
      "\n",
      "**Innovative Simplicity in Dynamic Batching**  \n",
      "The paper makes a compelling case for the value of simplicity in dynamic batching by introducing LWD, a fully online algorithm that adjusts batch sizes based solely on the outcome of the previous build. Adhering to Occam's Razor, LWD avoids reliance on historical data or offline models, instead using basic arithmetic operations and four guiding principles (Fallback, Retention, Factor, Customizability). This approach contrasts favorably with prior work by Bavand et al., which employs complex weighted failure rates and lookup tables. LWD’s five variants—Linear, Exponential, Random, Mixed, and MFU—demonstrate performance comparable to state-of-the-art methods while achieving a median 4.75% greater build savings than static batching. The algorithm’s lightweight nature and immediate adaptability make it particularly suitable for rapidly evolving CI environments.\n",
      "\n",
      "## Three Limitations\n",
      "\n",
      "**Over-Reliance on “Builds Saved” as a Proxy Metric**  \n",
      "While the paper demonstrates that LWD reduces the number of builds compared to static batching and TestAll baselines, its primary evaluation metric—“builds saved”—fails to capture the full operational cost of CI pipelines. The authors attempt to estimate time savings, but their approximation assumes linear batch durations and neglects parallel execution and inter-commit dependencies. As a result, the wall-clock time savings are not conclusively tied to LWD’s build reduction, and in some cases, baseline dynamic batching outperforms LWD in terms of feedback speed. This raises questions about the real-world impact of the proposed method on CI efficiency, particularly in environments where rapid feedback is critical.\n",
      "\n",
      "**Narrow Dataset and Tooling Scope**  \n",
      "The empirical evaluation is limited to large, Java-based open-source projects hosted on TravisCI with at least 2,000 commits. This restricts the generalizability of the findings to smaller or proprietary codebases, different programming languages (e.g., Python, C++), or alternative CI platforms such as Jenkins or GitHub Actions. Furthermore, the integration of CI-Skip rules, which are specific to Java-based testing frameworks, may not be directly transferable to other ecosystems. This narrow scope limits the broader applicability of LWD and suggests the need for further validation across a more diverse set of environments.\n",
      "\n",
      "**CI-Skip Integration Offers Marginal Gains**  \n",
      "Although the paper explores the integration of LWD with CI-Skip rules, the results indicate only minimal improvements—0.87% median build savings—when combining the two. Given that CI-Skip targets successful commits while dynamic batching primarily optimizes failure scenarios, the lack of synergy is understandable but underexplored. The authors do not propose alternative integration strategies or investigate complementary techniques, such as machine learning-based skip rules or post-batching skipping. This limits the paper’s contribution to a holistic optimization of CI systems, where both failure-driven batching and success-driven skipping could be synergistically leveraged.\n",
      "\n",
      "## Three Research Suggestions\n",
      "\n",
      "**Dynamic Optimization of LWD’s Configurable Parameters**  \n",
      "To further enhance LWD’s utility, future work should explore automated parameter tuning mechanisms that adapt LWD’s configuration (e.g., fallback thresholds, arithmetic factors) in real time based on project-specific metrics such as build duration trends and failure rate volatility. For example, a reinforcement learning approach or rule-based system could dynamically adjust the Factor Rule to optimize performance in changing environments. This would reduce the burden on engineers to manually select optimal settings and could improve LWD’s responsiveness to evolving project dynamics. Experimental comparisons between fixed and adaptive variants would provide valuable insights into the trade-offs between configurability and performance stability.\n",
      "\n",
      "**Integrated Prioritization of Build-Time and Build-Count Optimization**  \n",
      "Given the limitations of focusing solely on build count reduction, future research should develop a unified optimization framework that balances build-time efficiency and build-count savings. Such a framework could incorporate a cost function that weighs LWD’s build savings against the urgency of feedback delivery, particularly in high-stakes development contexts. This would involve lightweight monitoring of CI system metrics and dynamic switching between batching strategies (e.g., LWD during low-failure periods and more aggressive batching during high-failure periods). The goal would be to maximize both efficiency and developer productivity by aligning batching decisions with real-time operational priorities.\n",
      "\n",
      "**Cross-Language and Cross-Tool Validation of LWD**  \n",
      "To broaden the applicability of LWD, future studies should evaluate the algorithm in diverse language ecosystems (e.g., Python, C++) and CI platforms (e.g., Jenkins, GitHub Actions). This would require adapting LWD’s failure thresholds and fallback strategies to account for language-specific build behaviors and dependency structures. For instance, Python projects may exhibit different inter-commit dependencies due to their dynamic nature, potentially affecting batching efficiency. Recalibrating LWD’s parameters for these environments would help determine whether its local, failure-rate-driven updates are universally effective or require domain-specific refinements. This cross-platform validation would significantly enhance the practical relevance and generalizability of the proposed approach.\n",
      "\n",
      "📊 PROCESS ANALYTICS:\n",
      "--------------------------------------------------\n",
      "\n",
      "🔬 Component Analysis Results:\n",
      "\n",
      "  📋 Top Strengths:\n",
      "    1. Robust Empirical Evaluation of Batching Trade-offs\n",
      "    2. Customizable and Explainable Batching Policies\n",
      "    3. Innovative Simplicity in Dynamic Batching\n",
      "\n",
      "  📋 Top Limitations:\n",
      "    1. Over-Reliance on “Builds Saved” as a Proxy Metric\n",
      "    2. Narrow Dataset and Tooling Scope\n",
      "    3. CI-Skip Integration Offers Marginal Gains\n",
      "\n",
      "  📋 Top Suggestions:\n",
      "    1. Dynamic Optimization of LWD’s Configurable Parameters\n",
      "    2. Integrated Prioritization of Build-Time and Build-Count Optimization\n",
      "    3. Cross-Language and Cross-Tool Validation of LWD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word Count</td>\n",
       "      <td>1027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Character Count</td>\n",
       "      <td>7823.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paragraph Count</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence Count</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average Sentence Length</td>\n",
       "      <td>16.301587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Metric        Value\n",
       "0               Word Count  1027.000000\n",
       "1          Character Count  7823.000000\n",
       "2          Paragraph Count    13.000000\n",
       "3           Sentence Count    63.000000\n",
       "4  Average Sentence Length    16.301587"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️  Processing Summary:\n",
      "   • Total interactions: 20\n",
      "   • Successful interactions: 18\n",
      "   • Total processing time: 1110.37 seconds\n",
      "\n",
      "📊 Task Breakdown:\n",
      "   • critique_generation: 4 interactions\n",
      "   • extract_strengths: 1 interactions\n",
      "   • extract_limitations: 1 interactions\n",
      "   • extract_suggestions: 1 interactions\n",
      "   • rank_strengths: 4 interactions\n",
      "   • rank_limitations: 4 interactions\n",
      "   • rank_suggestions: 4 interactions\n",
      "   • final_synthesis: 1 interactions\n",
      "\n",
      "✅ ALL DONE! Check the generated files:\n",
      "   📄 ./results/final_critique.md - Submit this to Professor Adams\n",
      "   📋 ./results/full_report.md - Complete analysis report\n",
      "   📁 ./results/individual_responses/ - Individual LLM responses\n",
      "   📁 ./results/component_analysis/ - Detailed component analysis\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Execute Enhanced Pipeline\n",
    "# RUN THIS CELL TO START THE COMPLETE PROCESS\n",
    "\n",
    "try:\n",
    "    final_critique, full_report = run_complete_pipeline()\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 ENHANCED PIPELINE RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n📝 FINAL CRITIQUE (for Professor Adams):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(final_critique)\n",
    "    \n",
    "    print(\"\\n📊 PROCESS ANALYTICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Component analysis summary\n",
    "    component_summary = results_analyzer.create_component_analysis_summary()\n",
    "    print(\"\\n🔬 Component Analysis Results:\")\n",
    "    for component_type, selection in component_summary['final_selection'].items():\n",
    "        print(f\"\\n  📋 Top {component_type.title()}:\")\n",
    "        for i, item in enumerate(selection, 1):\n",
    "            print(f\"    {i}. {item['title']}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    try:\n",
    "        metrics = results_analyzer.create_synthesis_quality_metrics()\n",
    "        if metrics:\n",
    "            metrics_df = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "            display(metrics_df)\n",
    "        else:\n",
    "            print(\"📊 No metrics available - synthesis may have failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Metrics display error: {e}\")\n",
    "    \n",
    "    # Interaction log summary\n",
    "    print(f\"\\n⏱️  Processing Summary:\")\n",
    "    print(f\"   • Total interactions: {len(llm_client.interaction_log)}\")\n",
    "    print(f\"   • Successful interactions: {sum(1 for log in llm_client.interaction_log if log['success'])}\")\n",
    "    print(f\"   • Total processing time: {sum(log['duration'] for log in llm_client.interaction_log):.2f} seconds\")\n",
    "    \n",
    "    # Task breakdown\n",
    "    task_counts = {}\n",
    "    for log in llm_client.interaction_log:\n",
    "        task = log['task']\n",
    "        task_counts[task] = task_counts.get(task, 0) + 1\n",
    "    \n",
    "    print(f\"\\n📊 Task Breakdown:\")\n",
    "    for task, count in task_counts.items():\n",
    "        print(f\"   • {task}: {count} interactions\")\n",
    "    \n",
    "    print(\"\\n✅ ALL DONE! Check the generated files:\")\n",
    "    print(\"   📄 ./results/final_critique.md - Submit this to Professor Adams\")\n",
    "    print(\"   📋 ./results/full_report.md - Complete analysis report\")\n",
    "    print(\"   📁 ./results/individual_responses/ - Individual LLM responses\")\n",
    "    print(\"   📁 ./results/component_analysis/ - Detailed component analysis\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Enhanced pipeline failed: {e}\")\n",
    "    print(\"\\n🔍 Debugging information:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. API keys are correctly configured\")\n",
    "    print(\"2. briefing-doc.txt and faq.txt files exist in ./data/paper-3/\")\n",
    "    print(\"3. Internet connection is stable\")\n",
    "    print(\"4. All dependencies are properly installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff87a28-6d1f-4d92-b10d-282f22e42f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Interactive Critique Refinement Tool\n",
      "==================================================\n",
      "This tool allows you to refine the final critique using custom prompts.\n",
      "You can skip this step if you're satisfied with the current critique.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df42354dc5514850a3430b535540a8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=False, description='Skip refinement (use current critique as final)', style=Chec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11: Interactive Critique Refinement (Optional)\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_interactive_refinement_tool():\n",
    "    \"\"\"Create interactive tool for refining the critique with custom prompts\"\"\"\n",
    "    \n",
    "    # Check if we have a final critique\n",
    "    if not hasattr(review_system, 'final_synthesis') or not review_system.final_synthesis:\n",
    "        print(\"❌ No final critique available. Please run the pipeline first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"🔧 Interactive Critique Refinement Tool\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"This tool allows you to refine the final critique using custom prompts.\")\n",
    "    print(\"You can skip this step if you're satisfied with the current critique.\")\n",
    "    print()\n",
    "    \n",
    "    # Skip option\n",
    "    skip_widget = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Skip refinement (use current critique as final)',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Custom prompt input\n",
    "    custom_prompt = widgets.Textarea(\n",
    "        value=\"Please make the critique more concise while maintaining all key points and academic rigor.\",\n",
    "        description='Refinement Prompt:',\n",
    "        layout=widgets.Layout(width='100%', height='100px'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Current critique display\n",
    "    current_critique_display = widgets.Textarea(\n",
    "        value=review_system.final_synthesis,\n",
    "        description='Current Critique:',\n",
    "        layout=widgets.Layout(width='100%', height='300px'),\n",
    "        style={'description_width': 'initial'},\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Refined critique display\n",
    "    refined_critique_display = widgets.Textarea(\n",
    "        value=\"\",\n",
    "        description='Refined Critique:',\n",
    "        layout=widgets.Layout(width='100%', height='300px'),\n",
    "        style={'description_width': 'initial'},\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Control buttons\n",
    "    refine_button = widgets.Button(\n",
    "        description='Apply Refinement',\n",
    "        button_style='primary',\n",
    "        icon='magic'\n",
    "    )\n",
    "    \n",
    "    save_button = widgets.Button(\n",
    "        description='Save Refined Version',\n",
    "        button_style='success',\n",
    "        icon='save',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_skip_change(change):\n",
    "        \"\"\"Handle skip checkbox change\"\"\"\n",
    "        if change['new']:\n",
    "            custom_prompt.disabled = True\n",
    "            refine_button.disabled = True\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                print(\"✅ Skipping refinement. Using current critique as final version.\")\n",
    "        else:\n",
    "            custom_prompt.disabled = False\n",
    "            refine_button.disabled = False\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "    \n",
    "    def on_refine_click(b):\n",
    "        \"\"\"Handle refinement button click\"\"\"\n",
    "        if skip_widget.value:\n",
    "            return\n",
    "            \n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"🔧 Applying custom refinement...\")\n",
    "            print(f\"📝 Prompt: {custom_prompt.value[:100]}...\")\n",
    "            \n",
    "            # Create refinement prompt\n",
    "            refinement_prompt = f\"\"\"\n",
    "You are an expert academic writer. Please refine the following paper critique based on this specific instruction:\n",
    "\n",
    "INSTRUCTION: {custom_prompt.value}\n",
    "\n",
    "CURRENT CRITIQUE:\n",
    "{review_system.final_synthesis}\n",
    "\n",
    "Please provide an improved version that addresses the instruction while maintaining:\n",
    "- Academic rigor and professional tone\n",
    "- The required structure (3 strengths, 3 limitations, 3 suggestions)\n",
    "- All important technical details and examples\n",
    "- Appropriate length (1-2 pages)\n",
    "\n",
    "Return only the refined critique, no additional commentary.\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                refined_response = llm_client.query_qwen(refinement_prompt, \"refinement\")\n",
    "                refined_critique_display.value = refined_response\n",
    "                save_button.disabled = False\n",
    "                print(\"✅ Refinement completed!\")\n",
    "                print(f\"📊 Original length: {len(review_system.final_synthesis.split())} words\")\n",
    "                print(f\"📊 Refined length: {len(refined_response.split())} words\")\n",
    "                \n",
    "                # Update the review system's final synthesis\n",
    "                review_system.final_synthesis = refined_response\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Refinement failed: {e}\")\n",
    "    \n",
    "    def on_save_click(b):\n",
    "        \"\"\"Handle save button click\"\"\"\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"💾 Saving refined critique...\")\n",
    "            \n",
    "            try:\n",
    "                # Save refined version\n",
    "                refined_content = refined_critique_display.value\n",
    "                \n",
    "                # Save with timestamp\n",
    "                timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f'./results/refined_critique_{timestamp}.md'\n",
    "                \n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"# Refined Paper Critique\\n\")\n",
    "                    f.write(f\"**Paper:** {review_system.doc_processor.paper_title}\\n\")\n",
    "                    f.write(f\"**Refined:** {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                    f.write(f\"**Refinement Prompt:** {custom_prompt.value}\\n\\n\")\n",
    "                    f.write(refined_content)\n",
    "                \n",
    "                # Also update the main final critique file\n",
    "                # with open('./results/final_critique.md', 'w', encoding='utf-8') as f:\n",
    "                #     f.write(refined_content)\n",
    "                \n",
    "                print(f\"✅ Refined critique saved to: {filename}\")\n",
    "                # print(\"✅ Updated main final critique file: ./results/final_critique.md\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Save failed: {e}\")\n",
    "    \n",
    "    # Connect event handlers\n",
    "    skip_widget.observe(on_skip_change, names='value')\n",
    "    refine_button.on_click(on_refine_click)\n",
    "    save_button.on_click(on_save_click)\n",
    "    \n",
    "    # Layout\n",
    "    interface = widgets.VBox([\n",
    "        skip_widget,\n",
    "        widgets.HTML(\"<br>\"),\n",
    "        custom_prompt,\n",
    "        widgets.HTML(\"<br>\"),\n",
    "        widgets.HBox([refine_button, save_button]),\n",
    "        widgets.HTML(\"<br>\"),\n",
    "        current_critique_display,\n",
    "        widgets.HTML(\"<br>\"),\n",
    "        refined_critique_display,\n",
    "        widgets.HTML(\"<br>\"),\n",
    "        output_area\n",
    "    ])\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create and display the interactive tool\n",
    "try:\n",
    "    refinement_tool = create_interactive_refinement_tool()\n",
    "    if refinement_tool:\n",
    "        display(refinement_tool)\n",
    "except Exception as e:\n",
    "    print(f\"Could not create refinement tool: {e}\")\n",
    "    print(\"You can still use the final critique from the previous step.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffc173a-6da1-4d4b-acbe-0e593e218fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Creating enhanced submission package...\n",
      "==================================================\n",
      "✅ Enhanced submission package created!\n",
      "📁 Files generated:\n",
      "   - ./results/1_critique_document.md\n",
      "   - ./results/2_ai_collaboration_reflection.md\n",
      "   - ./results/3_interaction_logs.md\n",
      "   - ./results/README.md\n",
      "\n",
      "📁 Directory structure:\n",
      "   - ./results/individual_responses/ (Individual LLM responses)\n",
      "   - ./results/component_analysis/ (Complete component analysis)\n",
      "   - ./results/final_critique.md (Main deliverable)\n",
      "   - ./results/full_report.md (Complete analysis)\n",
      "\n",
      "🎯 READY FOR ENHANCED SUBMISSION!\n",
      "==================================================\n",
      "All files are prepared for Professor Adams with component-based analysis.\n",
      "The ./results/ directory contains everything you need.\n",
      "\n",
      "📧 Email attachments needed:\n",
      "   1. Zip the entire ./results/ directory (RECOMMENDED), OR\n",
      "   2. Send individual files:\n",
      "      - 1_critique_document.md (main critique)\n",
      "      - 2_ai_collaboration_reflection.md (enhanced reflection)\n",
      "      - 3_interaction_logs.md (detailed logs)\n",
      "\n",
      "✨ This submission showcases advanced AI collaboration with:\n",
      "   - Component-based analysis and ranking\n",
      "   - Multi-LLM consensus methodology\n",
      "   - Complete transparency and audit trail\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Enhanced Export and Submission Preparation\n",
    "class SubmissionPreparator:\n",
    "    def __init__(self, review_system, results_analyzer, llm_client):\n",
    "        self.review_system = review_system\n",
    "        self.results_analyzer = results_analyzer\n",
    "        self.llm_client = llm_client\n",
    "    \n",
    "    def create_submission_package(self):\n",
    "        \"\"\"Create complete submission package for Professor Adams\"\"\"\n",
    "        \n",
    "        print(\"📦 Creating enhanced submission package...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Main critique document\n",
    "        critique_content = f\"\"\"\n",
    "# Paper Critique\n",
    "\n",
    "**Paper Title:** {self.review_system.doc_processor.paper_title}\n",
    "**Paper URL:** {self.review_system.doc_processor.paper_url}\n",
    "**Student:** [Your Name Here]\n",
    "**Date:** {time.strftime('%Y-%m-%d')}\n",
    "\n",
    "---\n",
    "\n",
    "{self.review_system.final_synthesis}\n",
    "\n",
    "---\n",
    "\n",
    "*This critique was generated using an enhanced multi-agent AI system with component-based analysis, utilizing GPT-o4-mini, Gemini 2.0 Flash, Grok 4, and Qwen3-235B models.*\n",
    "\"\"\"\n",
    "        \n",
    "        # 2. Enhanced AI collaboration reflection\n",
    "        component_summary = self.results_analyzer.create_component_analysis_summary()\n",
    "        \n",
    "        reflection_content = f\"\"\"\n",
    "# AI Collaboration Reflection\n",
    "\n",
    "## Enhanced Approach and Strategy\n",
    "\n",
    "I developed a sophisticated multi-agent system with component-based analysis to tackle this paper review challenge, utilizing four different Large Language Models (LLMs) with a novel approach for extracting and ranking individual critique components.\n",
    "\n",
    "### Models Used:\n",
    "- **GPT-o4-mini**: For balanced reasoning, component extraction, and academic writing\n",
    "- **Gemini 2.0 Flash**: For diverse perspectives and component ranking\n",
    "- **Grok 4**: For creative insights and alternative component evaluation\n",
    "- **Qwen3-235B**: For final synthesis and refinement\n",
    "\n",
    "### Enhanced Methodology:\n",
    "\n",
    "1. **Document Processing**: Used NotebookLM to create comprehensive briefing and FAQ documents from the original paper\n",
    "\n",
    "2. **Multi-Agent Generation**: Each LLM independently generated a complete structured critique with standardized format\n",
    "\n",
    "3. **Component Extraction**: Used GPT to systematically extract all strengths, limitations, and research suggestions from individual responses\n",
    "   - **Strengths Extracted:** {component_summary['extraction_stats'].get('strengths', {}).get('total_extracted', 0)}\n",
    "   - **Limitations Extracted:** {component_summary['extraction_stats'].get('limitations', {}).get('total_extracted', 0)}\n",
    "   - **Suggestions Extracted:** {component_summary['extraction_stats'].get('suggestions', {}).get('total_extracted', 0)}\n",
    "\n",
    "4. **Multi-LLM Component Ranking**: All four models independently ranked each component type\n",
    "   - Randomized component order to eliminate bias\n",
    "   - Each LLM provided rankings based on academic rigor, specificity, and relevance\n",
    "   - Consensus ranking calculated using Borda count methodology\n",
    "\n",
    "5. **Duplicate Detection & Filtering**: LLMs identified and eliminated duplicate/similar components to ensure diversity\n",
    "\n",
    "6. **Top Component Selection**: Selected top 3 unique components from each category based on consensus ranking\n",
    "\n",
    "7. **Intelligent Synthesis**: Qwen synthesized the top components into a cohesive, flowing academic critique\n",
    "\n",
    "8. **Interactive Refinement**: Optional custom prompt-based refinement for final polishing\n",
    "\n",
    "### Technical Implementation:\n",
    "\n",
    "The system was built as a comprehensive Jupyter notebook with:\n",
    "- **Automated Component Analysis**: Systematic extraction and parsing of critique components\n",
    "- **Multi-LLM Consensus Ranking**: Objective component evaluation across multiple models\n",
    "- **Duplicate Detection**: Intelligent filtering to ensure component diversity\n",
    "- **Structured File Management**: Organized storage of all analysis stages\n",
    "- **Complete Audit Trail**: Full logging of all interactions and decisions\n",
    "\n",
    "### Process Statistics:\n",
    "- **Total LLM Interactions:** {len(self.llm_client.interaction_log)}\n",
    "- **Successful Interactions:** {sum(1 for log in self.llm_client.interaction_log if log['success'])}\n",
    "- **Total Processing Time:** {sum(log['duration'] for log in self.llm_client.interaction_log):.2f} seconds\n",
    "- **Component Extraction Tasks:** {len([log for log in self.llm_client.interaction_log if 'extract' in log['task']])}\n",
    "- **Component Ranking Tasks:** {len([log for log in self.llm_client.interaction_log if 'rank' in log['task']])}\n",
    "\n",
    "### Innovation in Approach:\n",
    "\n",
    "This approach represents a significant advancement over traditional multi-agent systems by:\n",
    "\n",
    "1. **Component-Level Analysis**: Instead of ranking entire responses, individual components are extracted and evaluated separately, allowing for more granular quality assessment\n",
    "\n",
    "2. **Multi-Stage Consensus**: Components undergo multiple rounds of evaluation (extraction → ranking → consensus → synthesis) ensuring highest quality selection\n",
    "\n",
    "3. **Bias Reduction**: Randomization of component order and multi-LLM evaluation reduces individual model biases\n",
    "\n",
    "4. **Quality Optimization**: Only the best components from each category are used in final synthesis, resulting in a critique that combines the strongest insights from all models\n",
    "\n",
    "### What Worked Exceptionally Well:\n",
    "\n",
    "- **Component extraction** provided much more granular control over content quality\n",
    "- **Multi-LLM ranking consensus** eliminated individual model biases effectively\n",
    "- **Duplicate detection** ensured diverse, non-redundant final selection\n",
    "- **Systematic file organization** enabled complete transparency and reproducibility\n",
    "- **Structured prompting** with standardized formats improved response parsing reliability\n",
    "\n",
    "### Challenges and Solutions:\n",
    "\n",
    "- **Challenge**: Complex parsing of structured responses from different LLMs\n",
    "  **Solution**: Implemented robust parsing with fallback mechanisms and validation\n",
    "\n",
    "- **Challenge**: Ensuring component ranking consistency across models\n",
    "  **Solution**: Used standardized ranking prompts with clear evaluation criteria\n",
    "\n",
    "- **Challenge**: Maintaining component context during extraction\n",
    "  **Solution**: Preserved full content and source information for each component\n",
    "\n",
    "- **Challenge**: Avoiding information loss during synthesis\n",
    "  **Solution**: Provided all top component details to synthesis model for complete integration\n",
    "\n",
    "This enhanced approach demonstrates how intelligent decomposition and multi-stage consensus can significantly improve AI-generated academic content quality while maintaining full transparency and reproducibility.\n",
    "\"\"\"\n",
    "        \n",
    "        # 3. Enhanced interaction logs\n",
    "        logs_content = self._generate_enhanced_interaction_logs()\n",
    "        \n",
    "        # Save all files\n",
    "        files_created = []\n",
    "        \n",
    "        with open('./results/1_critique_document.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(critique_content)\n",
    "            files_created.append('1_critique_document.md')\n",
    "        \n",
    "        with open('./results/2_ai_collaboration_reflection.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(reflection_content)\n",
    "            files_created.append('2_ai_collaboration_reflection.md')\n",
    "        \n",
    "        with open('./results/3_interaction_logs.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(logs_content)\n",
    "            files_created.append('3_interaction_logs.md')\n",
    "        \n",
    "        # Create enhanced README\n",
    "        readme_content = f\"\"\"\n",
    "# Enhanced Paper Critique Submission Package\n",
    "\n",
    "**Student:** [Your Name Here]  \n",
    "**Paper:** {self.review_system.doc_processor.paper_title}  \n",
    "**Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Method:** Component-Based Multi-Agent Analysis\n",
    "\n",
    "## Contents:\n",
    "\n",
    "### Main Deliverables:\n",
    "1. **1_critique_document.md** - Main critique document (1-2 pages)\n",
    "2. **2_ai_collaboration_reflection.md** - Enhanced AI collaboration reflection\n",
    "3. **3_interaction_logs.md** - Complete interaction logs\n",
    "\n",
    "### Supporting Files:\n",
    "4. **individual_responses/** - Individual LLM responses\n",
    "   - gpto4_mini_response.md\n",
    "   - gemini_2_flash_response.md\n",
    "   - grok_4_response.md\n",
    "   - qwen3_235b_response.md\n",
    "\n",
    "5. **component_analysis/** - Detailed component analysis\n",
    "   - strengths_extraction_raw.md, limitations_extraction_raw.md, suggestions_extraction_raw.md\n",
    "   - strengths_extracted.json, limitations_extracted.json, suggestions_extracted.json\n",
    "   - strengths_rankings.json, limitations_rankings.json, suggestions_rankings.json\n",
    "   - strengths_consensus.json, limitations_consensus.json, suggestions_consensus.json\n",
    "   - synthesis_details.json\n",
    "\n",
    "6. **final_critique.md** - Final synthesized critique\n",
    "7. **full_report.md** - Complete analysis report\n",
    "8. **README.md** - This file\n",
    "\n",
    "## Submission Summary:\n",
    "\n",
    "✅ Paper critique with 3 strengths, 3 limitations, 3 suggestions  \n",
    "✅ Enhanced AI collaboration reflection with component-based methodology  \n",
    "✅ Complete interaction logs from all LLM interactions  \n",
    "✅ Individual responses from each LLM model saved separately\n",
    "✅ Detailed component extraction and ranking analysis\n",
    "✅ Full transparency with complete audit trail\n",
    "✅ Paper URL: {self.review_system.doc_processor.paper_url}\n",
    "\n",
    "## Technical Details:\n",
    "\n",
    "- **Models Used:** GPT-o4-mini, Gemini 2.0 Flash, Grok 4, Qwen3-235B\n",
    "- **Methodology:** Enhanced component-based multi-agent analysis\n",
    "- **Total Interactions:** {len(self.llm_client.interaction_log)}\n",
    "- **Component Analysis Tasks:** {len([log for log in self.llm_client.interaction_log if any(task in log['task'] for task in ['extract', 'rank'])])}\n",
    "- **Processing Time:** {sum(log['duration'] for log in self.llm_client.interaction_log):.2f} seconds\n",
    "- **Final Word Count:** {len(self.review_system.final_synthesis.split())} words\n",
    "\n",
    "## Innovation Highlights:\n",
    "\n",
    "- ✨ Component-level extraction and analysis\n",
    "- ✨ Multi-LLM consensus ranking with duplicate detection\n",
    "- ✨ Borda count methodology for objective component selection\n",
    "- ✨ Complete audit trail of all analysis stages\n",
    "- ✨ Enhanced synthesis from top-ranked components only\n",
    "\n",
    "Ready for submission to Professor Adams.\n",
    "\"\"\"\n",
    "        \n",
    "        with open('./results/README.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(readme_content)\n",
    "            files_created.append('README.md')\n",
    "        \n",
    "        print(\"✅ Enhanced submission package created!\")\n",
    "        print(\"📁 Files generated:\")\n",
    "        for file in files_created:\n",
    "            print(f\"   - ./results/{file}\")\n",
    "        \n",
    "        print(\"\\n📁 Directory structure:\")\n",
    "        print(\"   - ./results/individual_responses/ (Individual LLM responses)\")\n",
    "        print(\"   - ./results/component_analysis/ (Complete component analysis)\")\n",
    "        print(\"   - ./results/final_critique.md (Main deliverable)\")\n",
    "        print(\"   - ./results/full_report.md (Complete analysis)\")\n",
    "        \n",
    "        return files_created\n",
    "    \n",
    "    def _generate_enhanced_interaction_logs(self) -> str:\n",
    "        \"\"\"Generate enhanced formatted interaction logs\"\"\"\n",
    "        log_content = f\"# Enhanced Complete Interaction Logs\\n\\n\"\n",
    "        log_content += f\"**Paper:** {self.review_system.doc_processor.paper_title}\\n\"\n",
    "        log_content += f\"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        log_content += f\"**Method:** Component-Based Multi-Agent Analysis\\n\\n\"\n",
    "        \n",
    "        # Enhanced summary statistics\n",
    "        task_breakdown = {}\n",
    "        for log in self.llm_client.interaction_log:\n",
    "            task = log['task']\n",
    "            task_breakdown[task] = task_breakdown.get(task, 0) + 1\n",
    "        \n",
    "        log_content += \"## Enhanced Summary Statistics\\n\"\n",
    "        log_content += f\"- **Total Interactions:** {len(self.llm_client.interaction_log)}\\n\"\n",
    "        log_content += f\"- **Successful Interactions:** {sum(1 for log in self.llm_client.interaction_log if log['success'])}\\n\"\n",
    "        log_content += f\"- **Failed Interactions:** {sum(1 for log in self.llm_client.interaction_log if not log['success'])}\\n\"\n",
    "        log_content += f\"- **Total Processing Time:** {sum(log['duration'] for log in self.llm_client.interaction_log):.2f} seconds\\n\\n\"\n",
    "        \n",
    "        log_content += \"### Task Breakdown:\\n\"\n",
    "        for task, count in task_breakdown.items():\n",
    "            log_content += f\"- **{task}:** {count} interactions\\n\"\n",
    "        \n",
    "        log_content += \"\\n## Detailed Interaction Log\\n\\n\"\n",
    "        \n",
    "        # Group interactions by task type\n",
    "        task_groups = {}\n",
    "        for interaction in self.llm_client.interaction_log:\n",
    "            task = interaction['task']\n",
    "            if task not in task_groups:\n",
    "                task_groups[task] = []\n",
    "            task_groups[task].append(interaction)\n",
    "        \n",
    "        for task, interactions in task_groups.items():\n",
    "            log_content += f\"### {task.replace('_', ' ').title()} Phase\\n\\n\"\n",
    "            \n",
    "            for i, interaction in enumerate(interactions):\n",
    "                log_content += f\"#### {task.replace('_', ' ').title()} {i+1}\\n\"\n",
    "                log_content += f\"- **Model:** {config.models[interaction['llm_id']]['name']}\\n\"\n",
    "                log_content += f\"- **Duration:** {interaction['duration']:.2f} seconds\\n\"\n",
    "                log_content += f\"- **Success:** {'✅ Yes' if interaction['success'] else '❌ No'}\\n\"\n",
    "                log_content += f\"- **Timestamp:** {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(interaction['start_time']))}\\n\\n\"\n",
    "        \n",
    "        # Add response summaries\n",
    "        log_content += \"# Response Summaries\\n\\n\"\n",
    "        for llm_id, response in self.review_system.responses.items():\n",
    "            log_content += f\"## {config.models[llm_id]['name']} Response\\n\"\n",
    "            log_content += f\"**Status:** {'Success' if not response.startswith('Error') else 'Failed'}\\n\"\n",
    "            log_content += f\"**Length:** {len(response)} characters\\n\"\n",
    "            log_content += f\"**Word Count:** {len(response.split())} words\\n\"\n",
    "            log_content += f\"**File:** ./individual_responses/{config.models[llm_id]['filename']}\\n\"\n",
    "            log_content += f\"**Preview:** {response[:300]}...\\n\\n\"\n",
    "        \n",
    "        # Add component analysis summary\n",
    "        component_summary = self.results_analyzer.create_component_analysis_summary()\n",
    "        log_content += \"# Component Analysis Summary\\n\\n\"\n",
    "        \n",
    "        for component_type, stats in component_summary['extraction_stats'].items():\n",
    "            log_content += f\"## {component_type.title()} Analysis\\n\"\n",
    "            log_content += f\"- **Total Extracted:** {stats['total_extracted']}\\n\"\n",
    "            log_content += f\"- **Unique Items:** {stats['unique_titles']}\\n\"\n",
    "            log_content += f\"- **Final Selection:** 3 items\\n\\n\"\n",
    "            \n",
    "            log_content += \"### Top Selected Items:\\n\"\n",
    "            for i, item in enumerate(component_summary['final_selection'][component_type], 1):\n",
    "                log_content += f\"{i}. **{item['title']}**\\n   {item['content_preview']}\\n\\n\"\n",
    "        \n",
    "        return log_content\n",
    "\n",
    "# Create enhanced submission package\n",
    "try:\n",
    "    preparator = SubmissionPreparator(review_system, results_analyzer, llm_client)\n",
    "    submission_files = preparator.create_submission_package()\n",
    "\n",
    "    print(\"\\n🎯 READY FOR ENHANCED SUBMISSION!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"All files are prepared for Professor Adams with component-based analysis.\")\n",
    "    print(\"The ./results/ directory contains everything you need.\")\n",
    "    print(\"\\n📧 Email attachments needed:\")\n",
    "    print(\"   1. Zip the entire ./results/ directory (RECOMMENDED), OR\")\n",
    "    print(\"   2. Send individual files:\")\n",
    "    print(\"      - 1_critique_document.md (main critique)\")\n",
    "    print(\"      - 2_ai_collaboration_reflection.md (enhanced reflection)\")\n",
    "    print(\"      - 3_interaction_logs.md (detailed logs)\")\n",
    "    print(\"\\n✨ This submission showcases advanced AI collaboration with:\")\n",
    "    print(\"   - Component-based analysis and ranking\")\n",
    "    print(\"   - Multi-LLM consensus methodology\")\n",
    "    print(\"   - Complete transparency and audit trail\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create enhanced submission package: {e}\")\n",
    "    print(\"Check that the pipeline completed successfully first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7935407,
     "sourceId": 12566062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7935462,
     "sourceId": 12566143,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (Automatic Critique)",
   "language": "python",
   "name": "automatic-critique"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
